# M√≠nimos

## **Optimizaci√≥n 1-D y extensi√≥n a m√∫ltiples dimensiones**

En **optimizaci√≥n 1-D** (una sola variable) ya vimos m√©todos como **Golden Search**, **Interpolaci√≥n parab√≥lica** y **Newton**, donde buscamos el m√≠nimo de una funci√≥n

$$
f : [a,b] \subset \mathbb{R} \to \mathbb{R}
$$

En estos casos, la funci√≥n tiene una sola entrada y una sola salida. El problema se reduce a encontrar el punto del intervalo $[a,b]$ donde $f$ alcanza su valor m√°s peque√±o.

Estos m√©todos funcionan bien en 1-D, pero **el mundo real rara vez es 1-D**. En Machine Learning, ingenier√≠a, econom√≠a, f√≠sica‚Ä¶ casi siempre trabajamos con **muchas variables** al mismo tiempo. Por eso debemos extender las ideas a funciones:

$$
f: \mathbb{R}^n \to \mathbb{R}
$$

Aqu√≠ $x \in \mathbb{R}^n$ es un vector con varias variables:

$$
x = (x_1, x_2, \dots, x_n)^T
$$

y $f(x)$ es la **funci√≥n objetivo** a minimizar.

### **Optimizaci√≥n no restringida vs restringida**

En programaci√≥n lineal o problemas como el m√©todo simplex, transporte o asignaciones, trabajamos **dentro de una regi√≥n factible** definida por restricciones: desigualdades, igualdades, cotas, etc. Esto se llama **optimizaci√≥n restringida**.

En cambio, en **optimizaci√≥n no restringida**, no imponemos l√≠mites: el conjunto factible $\Omega$ puede ser todo $\mathbb{R}^n$. El m√≠nimo puede estar **en cualquier punto** del espacio. Muchos m√©todos basados en el gradiente funcionan en este escenario.

‚ö†Ô∏è Ojo: aunque sea ‚Äúno restringido‚Äù, en la pr√°ctica $\Omega$ se **restringe al dominio de la funci√≥n**. Si $f$ tiene saltos, divisiones por cero o regiones no definidas, no podemos evaluar ni optimizar ah√≠.

### **Variables continuas vs discretas**

En este curso tratamos $x \in \mathbb{R}^n$, es decir, **variables continuas**. Existe otro mundo llamado **optimizaci√≥n discreta** (por ejemplo, escoger combinaciones o rutas) que requiere t√©cnicas distintas (b√∫squeda exhaustiva, branch & bound, heur√≠sticas, etc.).
En general, la optimizaci√≥n continua es m√°s ‚Äúsuave‚Äù y permite usar c√°lculo diferencial para encontrar m√≠nimos y m√°ximos.

## **Ejemplos importantes en optimizaci√≥n no restringida**

### 1. **Cociente de Rayleigh**

$$
\min_x \frac{x^\top A x}{x^\top x}, \quad A \text{ sim√©trica}
$$

* Si $A$ es la **matriz de covarianzas** de un conjunto de datos, este cociente mide la **varianza** de la nube de puntos en la direcci√≥n $x$.
* **Maximizarlo** ‚Üí direcci√≥n de mayor varianza (**primer componente principal** en PCA).
* **Minimizarlo** ‚Üí direcci√≥n de menor varianza.
* En PCA, las direcciones que buscamos corresponden a **autovectores** de $A$, y el cociente de Rayleigh est√° directamente ligado a los **autovalores**.

üí° **¬øPor qu√© es √∫til?** En an√°lisis de datos queremos reducir la dimensi√≥n qued√°ndonos con las direcciones que m√°s informaci√≥n (varianza) aportan. El cociente de Rayleigh es la herramienta matem√°tica para hallarlas.

### 2. **M√≠nimos cuadrados con regularizaci√≥n de Tikhonov**

$$
\min_x \sum_{i=1}^n (x_i - y_i)^2 \;+\; \lambda \sum_{i=1}^{n-1} (x_{i+1}-x_i)^2
$$

* Primer t√©rmino: mide **qu√© tan cerca est√°n las predicciones $x_i$** de los datos reales $y_i$ (error de ajuste).
* Segundo t√©rmino: penaliza cambios bruscos entre puntos consecutivos (fuerza la **suavidad**).
* $\lambda > 0$ controla el balance:

  * $\lambda$ peque√±o ‚Üí m√°s ajuste (riesgo de overfitting).
  * $\lambda$ grande ‚Üí m√°s suavidad (puede subajustar).
* Este es un caso particular de **regularizaci√≥n**, que es clave en Machine Learning para evitar que el modelo aprenda demasiado el ruido de los datos.

üí° **¬øPor qu√© es √∫til?** Porque casi todos los modelos reales deben generalizar bien, y eso se logra evitando oscilaciones excesivas en la predicci√≥n.

### 3. **Funci√≥n de Rosenbrock**

$$
\min_x \sum_{i=1}^{n-1} \big[(x_{i+1}-x_i^2)^2 + (1-x_i)^2\big]
$$

* Paisaje con un valle muy estrecho y curvado.
* El gradiente se hace muy peque√±o dentro del valle ‚Üí los m√©todos de descenso por gradiente avanzan muy lento.
* Se usa como **funci√≥n de prueba** para medir la eficiencia y robustez de algoritmos de optimizaci√≥n.

üí° **¬øPor qu√© es √∫til?** Porque es un ‚Äúcampo de entrenamiento‚Äù para ver si un m√©todo puede manejar superficies complicadas donde la convergencia no es trivial.

## **El gradiente: direcci√≥n de crecimiento y decrecimiento m√°ximo**

Para $f: \mathbb{R}^n \to \mathbb{R}$ diferenciable, el **gradiente** en un punto $p$ es:

$$
\nabla f(p) = \left( \frac{\partial f}{\partial x_1}(p), \dots, \frac{\partial f}{\partial x_n}(p) \right)^T
$$

* Indica **la direcci√≥n en la que $f$ crece m√°s r√°pido**.
* Magnitud del gradiente ‚Üí qu√© tan r√°pido crece en esa direcci√≥n.
* Direcci√≥n opuesta ‚Üí **descenso m√°s r√°pido** (base del **Gradient Descent**).

üí° **Interpretaci√≥n geom√©trica:** el gradiente es **perpendicular** a las curvas de nivel de la funci√≥n. Si te imaginas un mapa de monta√±as, el gradiente apunta cuesta arriba en la pendiente m√°s empinada.

Perfecto, lo voy a rehacer en un texto limpio, completo y bien explicado, manteniendo **todo lo que pusiste** pero orden√°ndolo y extendi√©ndolo para que se entienda, sin omitir nada y reforzando con los ‚Äú¬øpor qu√©?‚Äù, ‚Äú¬øpara qu√©?‚Äù y ‚Äú¬øqu√© significa?‚Äù.

## **Gradiente, ortogonalidad y optimizaci√≥n**

En optimizaci√≥n, el **gradiente** es una de las herramientas m√°s poderosas porque nos dice hacia d√≥nde se mueve m√°s r√°pidamente el valor de la funci√≥n.
Una propiedad fundamental: **el gradiente en un punto siempre es ortogonal (perpendicular)** a las **curvas de nivel** de la funci√≥n en ese punto.
Esto significa que si dibujamos las l√≠neas donde la funci√≥n toma un mismo valor (curvas de nivel), el gradiente apuntar√° siempre hacia afuera, en la direcci√≥n en la que el valor de la funci√≥n crece m√°s r√°pido.

### **Por qu√© importa la convexidad**

Cuando pasamos una funci√≥n a un algoritmo de optimizaci√≥n, idealmente queremos que sea **convexa**.
¬øPor qu√©? Porque una funci√≥n convexa tiene una √∫nica forma de ‚ÄúU‚Äù (en 1D) o un √∫nico ‚Äúvalle‚Äù (en m√°s dimensiones) y, por lo tanto, **un √∫nico m√≠nimo global**.
Esto garantiza que si seguimos la direcci√≥n de descenso, inevitablemente llegaremos al m√≠nimo buscado, sin riesgo de quedar atrapados en m√≠nimos locales falsos.

En funciones m√°s complicadas, con muchos m√≠nimos, la situaci√≥n cambia:

* Hay que dise√±ar estrategias, como **partir la b√∫squeda en intervalos** $[a,b]$, luego $[c,d]$, etc., para encontrar m√≠nimos locales por zonas.
* En problemas con muchas irregularidades, es necesario un **estudio preliminar**: graficar, analizar, detectar posibles regiones de inter√©s y decidir d√≥nde aplicar el algoritmo.

### **Tipos de funciones y su complejidad**

Las funciones que encontramos en la pr√°ctica pueden tener:

* Regiones planas ‚Üí el gradiente es muy peque√±o y los algoritmos avanzan lento.
* Discontinuidades ‚Üí no podemos calcular derivadas, se necesita otro enfoque.
* M√∫ltiples m√≠nimos ‚Üí riesgo de quedarse atrapado en un m√≠nimo local.
* Picos muy pronunciados ‚Üí pueden desestabilizar el algoritmo.

Por eso, **antes de lanzar un algoritmo a ciegas**, hay que entender la funci√≥n, graficarla, y anticipar d√≥nde pueden estar los problemas.

### **Maximizaci√≥n como minimizaci√≥n**

En optimizaci√≥n, **casi todo se formula como un problema de minimizaci√≥n**.
Si queremos **maximizar** una funci√≥n $g(x)$, basta con considerar $-g(x)$:

* El m√°ximo de $g$ ocurre en el mismo punto que el m√≠nimo de $-g$.
* As√≠ podemos usar cualquier algoritmo de minimizaci√≥n para resolver problemas de maximizaci√≥n.

### **Gradiente, magnitud y direcci√≥n**

Cuando calculamos el gradiente:

* La **direcci√≥n** indica hacia d√≥nde crece m√°s r√°pido la funci√≥n.
* La **magnitud** (longitud del vector gradiente) indica **cu√°n r√°pido crece**.

  * Magnitud grande ‚Üí cambios abruptos.
  * Magnitud peque√±a ‚Üí cambios suaves o regiones planas.

Esto es clave en algoritmos: si la magnitud $\|\nabla f(x)\|$ es muy peque√±a, podemos usarla como **criterio de paro** (hemos llegado a un punto donde el gradiente casi desaparece ‚Üí posible m√≠nimo).

### **Gradiente vs derivada (Jacobiano)**

En funciones $f: \mathbb{R}^n \to \mathbb{R}$:

* **Gradiente**: vector columna ($n \times 1$) con las derivadas parciales.

  $$
  \nabla f(x) =
  \begin{pmatrix}
  \frac{\partial f}{\partial x_1} \\
  \vdots \\
  \frac{\partial f}{\partial x_n}
  \end{pmatrix}
  $$
* **Derivada (Jacobiano)**: matriz fila ($1 \times n$) con las mismas entradas que el gradiente, pero escritas horizontalmente.

En funciones $f: \mathbb{R}^n \to \mathbb{R}^m$:

* El **Jacobiano** es una matriz $m \times n$, donde cada fila contiene las derivadas parciales de una de las salidas respecto a todas las variables de entrada.
* Si $m = 1$, el Jacobiano y el gradiente son lo mismo, salvo por la forma (fila vs columna).

**Regla de la cadena matricial**: para funciones compuestas $f(g(x))$,

$$
D(f \circ g)(p) = Df(g(p)) \cdot Dg(p)
$$

Se multiplican los Jacobianos como matrices.

### **Segunda derivada y Hessiano**

La segunda derivada en optimizaci√≥n multidimensional se representa con el **Hessiano**:

$$
H_f(p) =
\begin{pmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{pmatrix}
$$

Es una matriz $n \times n$ que describe la **curvatura** de la funci√≥n alrededor de un punto.

### **Autovalores y curvatura**

El Hessiano es sim√©trico en funciones suaves, y por tanto tiene:

* Autovectores ortogonales ‚Üí direcciones principales de curvatura.
* Autovalores reales ‚Üí indican si la curvatura va hacia arriba o hacia abajo en cada direcci√≥n.

Interpretaci√≥n:

* Todos los autovalores $> 0$ ‚Üí curvatura hacia arriba ‚Üí **m√≠nimo local**.
* Todos $< 0$ ‚Üí curvatura hacia abajo ‚Üí **m√°ximo local**.
* Mezcla de signos ‚Üí **punto de silla**.

### **M√≠nimos locales y globales**

* **M√≠nimo local** $x^*$:

  $$
  f(x^*) \le f(x) \quad \text{para todo } x \text{ en un radio } r \text{ alrededor de } x^*
  $$

  Es el m√°s bajo **solo en una vecindad**.

* **M√≠nimo global**:

  $$
  f(x^*) \le f(x) \quad \text{para todo } x \in \mathbb{R}^n
  $$

  Es el m√°s bajo **en todo el dominio**.

Tipos:

* **Estricto**: la desigualdad es estricta ($<$) salvo en el propio punto.
* **No estricto**: hay varios puntos con el mismo valor m√≠nimo.

Un m√≠nimo puede ser **aislado** si en una vecindad peque√±a no hay otros m√≠nimos con el mismo valor.

### **Dificultades comunes en optimizaci√≥n**

* M√∫ltiples m√≠nimos locales ‚Üí riesgo de quedarse atrapado.
* Regiones planas ‚Üí gradiente muy peque√±o, convergencia lenta.
* Funciones no diferenciables o discontinuas ‚Üí gradiente no definido.
* Picos o cambios bruscos ‚Üí pasos inestables.

Por eso, **siempre se recomienda estudiar y graficar** la funci√≥n antes de aplicar un algoritmo, para no confiar ciegamente en el resultado num√©rico.

### **Descenso por gradiente**

El m√©todo m√°s b√°sico para encontrar m√≠nimos locales:

1. Elegimos un punto inicial $x_0$.
2. Calculamos el gradiente $\nabla f(x_k)$.
3. Movemos el punto en direcci√≥n opuesta al gradiente:

   $$
   x_{k+1} = x_k - \alpha \, \nabla f(x_k)
   $$

   donde $\alpha$ es el **tama√±o de paso**.
4. Repetimos hasta que $\|\nabla f(x_k)\|$ sea suficientemente peque√±o (criterio de paro).

La elecci√≥n de $\alpha$ es clave:

* Muy grande ‚Üí riesgo de saltar el m√≠nimo.
* Muy peque√±a ‚Üí convergencia lenta.
