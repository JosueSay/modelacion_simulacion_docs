# MÃ­nimos

## **OptimizaciÃ³n 1-D y extensiÃ³n a mÃºltiples dimensiones**

En **optimizaciÃ³n 1-D** (una sola variable) ya vimos mÃ©todos como **Golden Search**, **InterpolaciÃ³n parabÃ³lica** y **Newton**, donde buscamos el mÃ­nimo de una funciÃ³n

$$
f : [a,b] \subset \mathbb{R} \to \mathbb{R}
$$

En estos casos, la funciÃ³n tiene una sola entrada y una sola salida. El problema se reduce a encontrar el punto del intervalo $[a,b]$ donde $f$ alcanza su valor mÃ¡s pequeÃ±o.

Estos mÃ©todos funcionan bien en 1-D, pero **el mundo real rara vez es 1-D**. En Machine Learning, ingenierÃ­a, economÃ­a, fÃ­sicaâ€¦ casi siempre trabajamos con **muchas variables** al mismo tiempo. Por eso debemos extender las ideas a funciones:

$$
f: \mathbb{R}^n \to \mathbb{R}
$$

AquÃ­ $x \in \mathbb{R}^n$ es un vector con varias variables:

$$
x = (x_1, x_2, \dots, x_n)^T
$$

y $f(x)$ es la **funciÃ³n objetivo** a minimizar.

### **OptimizaciÃ³n no restringida vs restringida**

En programaciÃ³n lineal o problemas como el mÃ©todo simplex, transporte o asignaciones, trabajamos **dentro de una regiÃ³n factible** definida por restricciones: desigualdades, igualdades, cotas, etc. Esto se llama **optimizaciÃ³n restringida**.

En cambio, en **optimizaciÃ³n no restringida**, no imponemos lÃ­mites: el conjunto factible $\Omega$ puede ser todo $\mathbb{R}^n$. El mÃ­nimo puede estar **en cualquier punto** del espacio. Muchos mÃ©todos basados en el gradiente funcionan en este escenario.

âš ï¸ Ojo: aunque sea â€œno restringidoâ€, en la prÃ¡ctica $\Omega$ se **restringe al dominio de la funciÃ³n**. Si $f$ tiene saltos, divisiones por cero o regiones no definidas, no podemos evaluar ni optimizar ahÃ­.

### **Variables continuas vs discretas**

En este curso tratamos $x \in \mathbb{R}^n$, es decir, **variables continuas**. Existe otro mundo llamado **optimizaciÃ³n discreta** (por ejemplo, escoger combinaciones o rutas) que requiere tÃ©cnicas distintas (bÃºsqueda exhaustiva, branch & bound, heurÃ­sticas, etc.).
En general, la optimizaciÃ³n continua es mÃ¡s â€œsuaveâ€ y permite usar cÃ¡lculo diferencial para encontrar mÃ­nimos y mÃ¡ximos.

## **Ejemplos importantes en optimizaciÃ³n no restringida**

### 1. **Cociente de Rayleigh**

$$
\min_x \frac{x^\top A x}{x^\top x}, \quad A \text{ simÃ©trica}
$$

* Si $A$ es la **matriz de covarianzas** de un conjunto de datos, este cociente mide la **varianza** de la nube de puntos en la direcciÃ³n $x$.
* **Maximizarlo** â†’ direcciÃ³n de mayor varianza (**primer componente principal** en PCA).
* **Minimizarlo** â†’ direcciÃ³n de menor varianza.
* En PCA, las direcciones que buscamos corresponden a **autovectores** de $A$, y el cociente de Rayleigh estÃ¡ directamente ligado a los **autovalores**.

ðŸ’¡ **Â¿Por quÃ© es Ãºtil?** En anÃ¡lisis de datos queremos reducir la dimensiÃ³n quedÃ¡ndonos con las direcciones que mÃ¡s informaciÃ³n (varianza) aportan. El cociente de Rayleigh es la herramienta matemÃ¡tica para hallarlas.

### 2. **MÃ­nimos cuadrados con regularizaciÃ³n de Tikhonov**

$$
\min_x \sum_{i=1}^n (x_i - y_i)^2 \;+\; \lambda \sum_{i=1}^{n-1} (x_{i+1}-x_i)^2
$$

* Primer tÃ©rmino: mide **quÃ© tan cerca estÃ¡n las predicciones $x_i$** de los datos reales $y_i$ (error de ajuste).
* Segundo tÃ©rmino: penaliza cambios bruscos entre puntos consecutivos (fuerza la **suavidad**).
* $\lambda > 0$ controla el balance:

  * $\lambda$ pequeÃ±o â†’ mÃ¡s ajuste (riesgo de overfitting).
  * $\lambda$ grande â†’ mÃ¡s suavidad (puede subajustar).
* Este es un caso particular de **regularizaciÃ³n**, que es clave en Machine Learning para evitar que el modelo aprenda demasiado el ruido de los datos.

ðŸ’¡ **Â¿Por quÃ© es Ãºtil?** Porque casi todos los modelos reales deben generalizar bien, y eso se logra evitando oscilaciones excesivas en la predicciÃ³n.

### 3. **FunciÃ³n de Rosenbrock**

$$
\min_x \sum_{i=1}^{n-1} \big[(x_{i+1}-x_i^2)^2 + (1-x_i)^2\big]
$$

* Paisaje con un valle muy estrecho y curvado.
* El gradiente se hace muy pequeÃ±o dentro del valle â†’ los mÃ©todos de descenso por gradiente avanzan muy lento.
* Se usa como **funciÃ³n de prueba** para medir la eficiencia y robustez de algoritmos de optimizaciÃ³n.

ðŸ’¡ **Â¿Por quÃ© es Ãºtil?** Porque es un â€œcampo de entrenamientoâ€ para ver si un mÃ©todo puede manejar superficies complicadas donde la convergencia no es trivial.

## **El gradiente: direcciÃ³n de crecimiento y decrecimiento mÃ¡ximo**

Para $f: \mathbb{R}^n \to \mathbb{R}$ diferenciable, el **gradiente** en un punto $p$ es:

$$
\nabla f(p) = \left( \frac{\partial f}{\partial x_1}(p), \dots, \frac{\partial f}{\partial x_n}(p) \right)^T
$$

* Indica **la direcciÃ³n en la que $f$ crece mÃ¡s rÃ¡pido**.
* Magnitud del gradiente â†’ quÃ© tan rÃ¡pido crece en esa direcciÃ³n.
* DirecciÃ³n opuesta â†’ **descenso mÃ¡s rÃ¡pido** (base del **Gradient Descent**).

ðŸ’¡ **InterpretaciÃ³n geomÃ©trica:** el gradiente es **perpendicular** a las curvas de nivel de la funciÃ³n. Si te imaginas un mapa de montaÃ±as, el gradiente apunta cuesta arriba en la pendiente mÃ¡s empinada.

Perfecto, lo voy a rehacer en un texto limpio, completo y bien explicado, manteniendo **todo lo que pusiste** pero ordenÃ¡ndolo y extendiÃ©ndolo para que se entienda, sin omitir nada y reforzando con los â€œÂ¿por quÃ©?â€, â€œÂ¿para quÃ©?â€ y â€œÂ¿quÃ© significa?â€.

## **Gradiente, ortogonalidad y optimizaciÃ³n**

En optimizaciÃ³n, el **gradiente** es una de las herramientas mÃ¡s poderosas porque nos dice hacia dÃ³nde se mueve mÃ¡s rÃ¡pidamente el valor de la funciÃ³n.
Una propiedad fundamental: **el gradiente en un punto siempre es ortogonal (perpendicular)** a las **curvas de nivel** de la funciÃ³n en ese punto.
Esto significa que si dibujamos las lÃ­neas donde la funciÃ³n toma un mismo valor (curvas de nivel), el gradiente apuntarÃ¡ siempre hacia afuera, en la direcciÃ³n en la que el valor de la funciÃ³n crece mÃ¡s rÃ¡pido.

### **Por quÃ© importa la convexidad**

Cuando pasamos una funciÃ³n a un algoritmo de optimizaciÃ³n, idealmente queremos que sea **convexa**.
Â¿Por quÃ©? Porque una funciÃ³n convexa tiene una Ãºnica forma de â€œUâ€ (en 1D) o un Ãºnico â€œvalleâ€ (en mÃ¡s dimensiones) y, por lo tanto, **un Ãºnico mÃ­nimo global**.
Esto garantiza que si seguimos la direcciÃ³n de descenso, inevitablemente llegaremos al mÃ­nimo buscado, sin riesgo de quedar atrapados en mÃ­nimos locales falsos.

En funciones mÃ¡s complicadas, con muchos mÃ­nimos, la situaciÃ³n cambia:

* Hay que diseÃ±ar estrategias, como **partir la bÃºsqueda en intervalos** $[a,b]$, luego $[c,d]$, etc., para encontrar mÃ­nimos locales por zonas.
* En problemas con muchas irregularidades, es necesario un **estudio preliminar**: graficar, analizar, detectar posibles regiones de interÃ©s y decidir dÃ³nde aplicar el algoritmo.

### **Tipos de funciones y su complejidad**

Las funciones que encontramos en la prÃ¡ctica pueden tener:

* Regiones planas â†’ el gradiente es muy pequeÃ±o y los algoritmos avanzan lento.
* Discontinuidades â†’ no podemos calcular derivadas, se necesita otro enfoque.
* MÃºltiples mÃ­nimos â†’ riesgo de quedarse atrapado en un mÃ­nimo local.
* Picos muy pronunciados â†’ pueden desestabilizar el algoritmo.

Por eso, **antes de lanzar un algoritmo a ciegas**, hay que entender la funciÃ³n, graficarla, y anticipar dÃ³nde pueden estar los problemas.

### **MaximizaciÃ³n como minimizaciÃ³n**

En optimizaciÃ³n, **casi todo se formula como un problema de minimizaciÃ³n**.
Si queremos **maximizar** una funciÃ³n $g(x)$, basta con considerar $-g(x)$:

* El mÃ¡ximo de $g$ ocurre en el mismo punto que el mÃ­nimo de $-g$.
* AsÃ­ podemos usar cualquier algoritmo de minimizaciÃ³n para resolver problemas de maximizaciÃ³n.

### **Gradiente, magnitud y direcciÃ³n**

Cuando calculamos el gradiente:

* La **direcciÃ³n** indica hacia dÃ³nde crece mÃ¡s rÃ¡pido la funciÃ³n.
* La **magnitud** (longitud del vector gradiente) indica **cuÃ¡n rÃ¡pido crece**.

  * Magnitud grande â†’ cambios abruptos.
  * Magnitud pequeÃ±a â†’ cambios suaves o regiones planas.

Esto es clave en algoritmos: si la magnitud $\|\nabla f(x)\|$ es muy pequeÃ±a, podemos usarla como **criterio de paro** (hemos llegado a un punto donde el gradiente casi desaparece â†’ posible mÃ­nimo).

### **Gradiente vs derivada (Jacobiano)**

En funciones $f: \mathbb{R}^n \to \mathbb{R}$:

* **Gradiente**: vector columna ($n \times 1$) con las derivadas parciales.

  $$
  \nabla f(x) =
  \begin{pmatrix}
  \frac{\partial f}{\partial x_1} \\
  \vdots \\
  \frac{\partial f}{\partial x_n}
  \end{pmatrix}
  $$
* **Derivada (Jacobiano)**: matriz fila ($1 \times n$) con las mismas entradas que el gradiente, pero escritas horizontalmente.

En funciones $f: \mathbb{R}^n \to \mathbb{R}^m$:

* El **Jacobiano** es una matriz $m \times n$, donde cada fila contiene las derivadas parciales de una de las salidas respecto a todas las variables de entrada.
* Si $m = 1$, el Jacobiano y el gradiente son lo mismo, salvo por la forma (fila vs columna).

**Regla de la cadena matricial**: para funciones compuestas $f(g(x))$,

$$
D(f \circ g)(p) = Df(g(p)) \cdot Dg(p)
$$

Se multiplican los Jacobianos como matrices.

### **Segunda derivada y Hessiano**

La segunda derivada en optimizaciÃ³n multidimensional se representa con el **Hessiano**:

$$
H_f(p) =
\begin{pmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{pmatrix}
$$

Es una matriz $n \times n$ que describe la **curvatura** de la funciÃ³n alrededor de un punto.

### **Autovalores y curvatura**

El Hessiano es simÃ©trico en funciones suaves, y por tanto tiene:

* Autovectores ortogonales â†’ direcciones principales de curvatura.
* Autovalores reales â†’ indican si la curvatura va hacia arriba o hacia abajo en cada direcciÃ³n.

InterpretaciÃ³n:

* Todos los autovalores $> 0$ â†’ curvatura hacia arriba â†’ **mÃ­nimo local**.
* Todos $< 0$ â†’ curvatura hacia abajo â†’ **mÃ¡ximo local**.
* Mezcla de signos â†’ **punto de silla**.

### **MÃ­nimos locales y globales**

* **MÃ­nimo local** $x^*$:

  $$
  f(x^*) \le f(x) \quad \text{para todo } x \text{ en un radio } r \text{ alrededor de } x^*
  $$

  Es el mÃ¡s bajo **solo en una vecindad**.

* **MÃ­nimo global**:

  $$
  f(x^*) \le f(x) \quad \text{para todo } x \in \mathbb{R}^n
  $$

  Es el mÃ¡s bajo **en todo el dominio**.

Tipos:

* **Estricto**: la desigualdad es estricta ($<$) salvo en el propio punto.
* **No estricto**: hay varios puntos con el mismo valor mÃ­nimo.

Un mÃ­nimo puede ser **aislado** si en una vecindad pequeÃ±a no hay otros mÃ­nimos con el mismo valor.

### **Dificultades comunes en optimizaciÃ³n**

* MÃºltiples mÃ­nimos locales â†’ riesgo de quedarse atrapado.
* Regiones planas â†’ gradiente muy pequeÃ±o, convergencia lenta.
* Funciones no diferenciables o discontinuas â†’ gradiente no definido.
* Picos o cambios bruscos â†’ pasos inestables.

Por eso, **siempre se recomienda estudiar y graficar** la funciÃ³n antes de aplicar un algoritmo, para no confiar ciegamente en el resultado numÃ©rico.

### **Descenso por gradiente**

El mÃ©todo mÃ¡s bÃ¡sico para encontrar mÃ­nimos locales:

1. Elegimos un punto inicial $x_0$.
2. Calculamos el gradiente $\nabla f(x_k)$.
3. Movemos el punto en direcciÃ³n opuesta al gradiente:

   $$
   x_{k+1} = x_k - \alpha \, \nabla f(x_k)
   $$

   donde $\alpha$ es el **tamaÃ±o de paso**.
4. Repetimos hasta que $\|\nabla f(x_k)\|$ sea suficientemente pequeÃ±o (criterio de paro).

La elecciÃ³n de $\alpha$ es clave:

* Muy grande â†’ riesgo de saltar el mÃ­nimo.
* Muy pequeÃ±a â†’ convergencia lenta.
